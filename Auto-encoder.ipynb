{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e606b0eb",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "095059c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math # for sqrt, log, exponentials\n",
    "import numpy as np # for vectorization and array\n",
    "import random # for random simulation\n",
    "import pandas as pd # for dataframe visualization\n",
    "import matplotlib.pyplot as plt # for plotting data in a graph\n",
    "import copy # for making predictions\n",
    "from collections import OrderedDict # ordering dictionaries\n",
    "import warnings # no annoying warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') # to ignore numpy's warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639c1d31",
   "metadata": {},
   "source": [
    "# Problem 1: Auto-Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "980d5814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_point(sigma):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Generates a data point of at 30 dimensions.\n",
    "    \n",
    "    Parameters:\n",
    "    sigma - a float number that alters our output, and adds more \n",
    "    noise (this should hinder the performance of our model)\n",
    "    \n",
    "    Returns:\n",
    "    feature_vector - a list with a length of (dimensions + 1)\n",
    "    where all elements are features\n",
    "    \"\"\"\n",
    "    \n",
    "    # intialize a feature vector of zeros\n",
    "    feature_vector = np.zeros(30)\n",
    "    \n",
    "    # modifying x1\n",
    "    feature_vector[0] = np.random.normal(0,1)\n",
    "    \n",
    "    # creating x4, x7, x10, x13, ... , x28\n",
    "    indices_to_modify = np.array(list(range(4,28+3,3))) - 1\n",
    "    for index in indices_to_modify:\n",
    "        feature_vector[index] = feature_vector[index - 3] + np.random.normal(0,sigma**2)\n",
    "    \n",
    "    # modifying x2\n",
    "    feature_vector[1] = feature_vector[0] + np.random.normal(0,sigma**2)\n",
    "    \n",
    "    # creating x5, x8, x11, ... , x29\n",
    "    indices_to_modify = np.array(list(range(5,29+3,3))) - 1\n",
    "    for index in indices_to_modify:\n",
    "        feature_vector[index] = feature_vector[index - 3] + np.random.normal(0,sigma**2)\n",
    "    \n",
    "    # modifying x3\n",
    "    feature_vector[2] = feature_vector[0] + np.random.normal(0,sigma**2)\n",
    "    \n",
    "    # creating x6, x9, x12, x15, ... , x30\n",
    "    indices_to_modify = np.array(list(range(6,30+3,3))) - 1\n",
    "    for index in indices_to_modify:\n",
    "        feature_vector[index] = feature_vector[index - 3] + np.random.normal(0,sigma**2)\n",
    "    \n",
    "    # adding the bias term\n",
    "    feature_vector = list(feature_vector)\n",
    "    feature_vector.insert(0,1)\n",
    "    \n",
    "    return np.array(feature_vector)\n",
    "\n",
    "def generate_train_data_set(training_data_size = 5000, sigma = 0.10):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    To use the generate_data_point function to generate training\n",
    "    data \n",
    "    \n",
    "    Parameters:\n",
    "    training_data_size - an integer specifying how many training data points\n",
    "    you would like to generate\n",
    "    \n",
    "    sigma - a float number that alters our output\n",
    "    \n",
    "    Returns:\n",
    "    x_train - ndarray with shape of ((dimensions + 1) x number of data points)\n",
    "    \"\"\"\n",
    "    \n",
    "    # intialize our test and training data\n",
    "    training_data = []\n",
    "    \n",
    "    # generating the training data\n",
    "    for _ in range(0,training_data_size):\n",
    "        training_data.append(generate_data_point(sigma))\n",
    "        \n",
    "    # putting our generated data into a numpy ndarray\n",
    "    x_train = np.array(training_data)\n",
    "\n",
    "    return x_train\n",
    "\n",
    "# doing this so we do not have to calculate e\n",
    "# everytime we run our activation function tanh\n",
    "e = math.e\n",
    "\n",
    "def tanh(z):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Our activation function in the hidden layer\n",
    "    \n",
    "    Parameters:\n",
    "    z - (30 x 1) vector containing random float values\n",
    "    \n",
    "    Returns:\n",
    "    A value used for learning in the autoencoder\n",
    "    \"\"\"\n",
    "    pos_power = e ** z\n",
    "    neg_power = e ** -z\n",
    "    \n",
    "    return (pos_power - neg_power) / (pos_power + neg_power)\n",
    "\n",
    "def ReLU(z):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Our activation function in our output layer\n",
    "    \n",
    "    Parameters:\n",
    "    z - (HIDDEN_NODES x 1) vector containing random float values\n",
    "    \n",
    "    Returns:\n",
    "    A value without bounds (our predictions)\n",
    "    \"\"\"\n",
    "    \n",
    "    return max(0,max(z))\n",
    "\n",
    "def calculate_loss(x_train, x_predicted):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    calculates the loss between the train data points\n",
    "    and the predicted data points\n",
    "    \n",
    "    Parameters:\n",
    "    x_train - (5000 x 30) dimensional array\n",
    "    x_predicted - (5000 x 30) dimensional array\n",
    "    \n",
    "    Returns:\n",
    "    loss - a float value indicating our error\n",
    "    \"\"\"\n",
    "    \n",
    "    # converting to numpy arrays\n",
    "    x_train = np.array(x_train)\n",
    "    x_predicted = np.array(x_predicted)\n",
    "    \n",
    "    # number of data points\n",
    "    N = len(x_train)\n",
    "    \n",
    "    # calculating the loss\n",
    "    loss = (1 / N) * np.sum( (np.linalg.norm((x_train - x_predicted)))**2 )\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def xavier_initialization(INPUT_NODES, HIDDEN_NODES, OUTPUT_NODES):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "    INPUT_NODES - an integer representing the number of input nodes in the first layer\n",
    "    HIDDEN_NODES - an integer representing the number if hidden nodes in the second layer\n",
    "    OUTPUT_NODES - an integer representing the number of output nodes in the third layer\n",
    "    \n",
    "    Returns:\n",
    "    weights - a dictionary that holds two ndarrays (one for each layer except the output layer)\n",
    "    \n",
    "    Purpose:\n",
    "    To intialize weights for our neural network, but in this case\n",
    "    our autoencoder\n",
    "    \"\"\"\n",
    "\n",
    "    # to hold the weights of the layers\n",
    "    weights = {\"Layer 1\":None,\"Layer 2\":None}\n",
    "\n",
    "    # building our w_vector - first build a temporary vector with all weights equal to one \n",
    "    # plus one for the bias weight\n",
    "    layer_1_weights = np.ones( ((INPUT_NODES + 1) * HIDDEN_NODES) )\n",
    "    layer_2_weights = np.ones( ((HIDDEN_NODES + 1) * OUTPUT_NODES) )\n",
    "\n",
    "    # initializing the weights with Xavier intialization\n",
    "    for index in range(0, len(layer_1_weights)):\n",
    "            \n",
    "        num_in = 0 # number of in nodes\n",
    "        num_out = HIDDEN_NODES # number of out nodes, FC layers\n",
    "\n",
    "        a = -1 * math.sqrt(6/(num_in + num_out)) # beginning of interval\n",
    "        b = -a # end of interval \n",
    "\n",
    "        new_weight = np.random.uniform(a,b) # calculating our new weight\n",
    "        layer_1_weights[index] = new_weight # putting our new weight into our weight vector\n",
    "        \n",
    "     # initializing the weights with Xavier intialization\n",
    "    for index in range(0, len(layer_2_weights)):\n",
    "\n",
    "        num_in = INPUT_NODES # number of in nodes\n",
    "        num_out = HIDDEN_NODES # number of out nodes, FC layers\n",
    "\n",
    "        a = -1 * math.sqrt(6/(num_in + num_out)) # beginning of interval\n",
    "        b = -a # end of interval \n",
    "\n",
    "        new_weight = np.random.uniform(a,b) # calculating our new weight\n",
    "\n",
    "        layer_2_weights[index] = new_weight # putting our new weight into our weight vector\n",
    "        \n",
    "    # assigning the weights to the weight dictionary\n",
    "    weights[\"Layer 1\"] = np.array(np.split(layer_1_weights, INPUT_NODES + 1))\n",
    "    weights[\"Layer 2\"] = np.array(np.split(layer_2_weights, HIDDEN_NODES + 1))\n",
    "    \n",
    "    # returning our weight dictionary\n",
    "    return weights \n",
    "\n",
    "def forward_propogation(input_layer, weights):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    input_layer - ndarray of shape ((1 + INPUT_NODES) x 1)\n",
    "    weights - a dictionary with ndarray of weights\n",
    "    HIDDEN_NODES - an integer representing the number if hidden nodes in the second layer\n",
    "    \n",
    "    Returns:\n",
    "    hidden_layer - ndarray of shape (1 + HIDDEN_NODES x 1)\n",
    "    output_layer - ndarray of shape (INPUT_NODES x 1)\n",
    "    \n",
    "    Purpose:\n",
    "    To compute a new hidden layer based off the weights of our model\n",
    "    To compute a new output layer with the newly computed hidden layer\n",
    "    \"\"\"    \n",
    "    # This function is vectorized using numpy\n",
    "    # for incredibly fast computation!!\n",
    "\n",
    "    # applying our weights to the input layer via dot product and the tanh activation function + bias term\n",
    "    hidden_layer = np.insert(tanh(np.dot(input_layer,weights['Layer 1'])), 0 , 1)\n",
    "\n",
    "    # applying our last weights\n",
    "    output_layer = np.dot(hidden_layer,weights['Layer 2'])\n",
    "\n",
    "    return (hidden_layer, output_layer)\n",
    "\n",
    "def back_propogation(input_layer, hidden_layer, output_layer, weights, alpha):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    All the layers of the neural network and\n",
    "    the learning rate alpha\n",
    "    \n",
    "    Returns:\n",
    "    new_weights = the new computed weights of our model\n",
    "    that minimize our error\n",
    "    \n",
    "    Purpose:\n",
    "    To update the weights of our model\n",
    "    using SGD (stochastic gradient descent)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ~almost vectorized~ but not quite\n",
    "    \n",
    "    # updating the second set of weights (before the output)\n",
    "    derivative_loss_to_node = (2 * (output_layer - input_layer[1:]))\n",
    "    \n",
    "    for index in range(0,len(hidden_layer)):\n",
    "        weights['Layer 2'][index] -= alpha * hidden_layer[index] * derivative_loss_to_node\n",
    "\n",
    "    # updating the first set of weights\n",
    "    derivative_first_weight = (1 - tanh(np.dot(input_layer,weights['Layer 1'])) ** 2)\n",
    "    \n",
    "    for index in range(0,len(hidden_layer)):\n",
    "        weights['Layer 1'][index] -= (alpha * derivative_first_weight * input_layer[index] * np.dot(derivative_loss_to_node,weights['Layer 2'][index]))\n",
    "        \n",
    "    return weights\n",
    "\n",
    "def run_experiment(x_train, num_hidden_layers, trials):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    Purpose:\n",
    "    \"\"\"\n",
    "    original_data = []\n",
    "    predictions = []\n",
    "    \n",
    "    # random initialization - Xavier Edition\n",
    "    weights = xavier_initialization(INPUT_NODES, HIDDEN_NODES, OUTPUT_NODES)\n",
    "    \n",
    "    # picking random points in the training data\n",
    "    for _ in range (0,trials):\n",
    "\n",
    "        input_layer = random.choice(x_train)\n",
    "\n",
    "        hidden_layer, output_layer = forward_propogation(input_layer, weights)\n",
    "        weights = back_propogation(input_layer, hidden_layer, output_layer, weights, alpha)\n",
    "\n",
    "        # to calculate the loss - no counting the bias term\n",
    "        original_data.append(input_layer[1:])\n",
    "        predictions.append(output_layer)\n",
    "\n",
    "    loss = calculate_loss(original_data,predictions)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9674de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NODES = 30 # input layer\n",
    "HIDDEN_NODES = 1 # hidden layer\n",
    "OUTPUT_NODES = 30 # output layer \n",
    "alpha = 0.0001 # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48c56f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate our data for training\n",
    "x_train = generate_train_data_set(sigma = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cbf58671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2292.064440524413"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment(x_train, num_hidden_layers = HIDDEN_NODES, trials = 10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
