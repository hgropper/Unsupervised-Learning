{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d28bfb",
   "metadata": {},
   "source": [
    "# Name: Harrison Gropper\n",
    "# Date: 12/13/2022\n",
    "# Section: 01:198:461:02\n",
    "# Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45f6395",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f6052b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math # for sqrt, log, exponentials\n",
    "import numpy as np # for vectorization and array\n",
    "import random # for random simulation\n",
    "import pandas as pd # for dataframe visualization\n",
    "import matplotlib.pyplot as plt # for plotting data in a graph\n",
    "import copy # for making predictions\n",
    "from collections import OrderedDict # ordering dictionaries\n",
    "import warnings # no annoying warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') # to ignore numpy's warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627f8b22",
   "metadata": {},
   "source": [
    "# Problem 1: Auto-Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56258bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_point(sigma):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Generates a data point of at 30 dimensions.\n",
    "    \n",
    "    Parameters:\n",
    "    sigma - a float number that alters our output, and adds more \n",
    "    noise (this should hinder the performance of our model)\n",
    "    \n",
    "    Returns:\n",
    "    feature_vector - a list with a length of (dimensions + 1)\n",
    "    where all elements are features\n",
    "    \"\"\"\n",
    "    \n",
    "    # intialize a feature vector of zeros\n",
    "    feature_vector = np.zeros(30)\n",
    "    \n",
    "    # modifying x1\n",
    "    feature_vector[0] = np.random.normal(0,1)\n",
    "    \n",
    "    # creating x4, x7, x10, x13, ... , x28\n",
    "    indices_to_modify = np.array(list(range(4,28+3,3))) - 1\n",
    "    for index in indices_to_modify:\n",
    "        feature_vector[index] = feature_vector[index - 3] + np.random.normal(0,sigma**2)\n",
    "    \n",
    "    # modifying x2\n",
    "    feature_vector[1] = feature_vector[0] + np.random.normal(0,sigma**2)\n",
    "    \n",
    "    # creating x5, x8, x11, ... , x29\n",
    "    indices_to_modify = np.array(list(range(5,29+3,3))) - 1\n",
    "    for index in indices_to_modify:\n",
    "        feature_vector[index] = feature_vector[index - 3] + np.random.normal(0,sigma**2)\n",
    "    \n",
    "    # modifying x3\n",
    "    feature_vector[2] = feature_vector[0] + np.random.normal(0,sigma**2)\n",
    "    \n",
    "    # creating x6, x9, x12, x15, ... , x30\n",
    "    indices_to_modify = np.array(list(range(6,30+3,3))) - 1\n",
    "    for index in indices_to_modify:\n",
    "        feature_vector[index] = feature_vector[index - 3] + np.random.normal(0,sigma**2)\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "def generate_train_data_set(training_data_size = 5000, sigma = 0.10):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    To use the generate_data_point function to generate training\n",
    "    data \n",
    "    \n",
    "    Parameters:\n",
    "    training_data_size - an integer specifying how many training data points\n",
    "    you would like to generate\n",
    "    \n",
    "    sigma - a float number that alters our output\n",
    "    \n",
    "    Returns:\n",
    "    x_train - ndarray with shape of (dimensions x number of data points)\n",
    "    \"\"\"\n",
    "    \n",
    "    # intialize our test and training data\n",
    "    training_data = []\n",
    "    \n",
    "    # generating the training data\n",
    "    for _ in range(0,training_data_size):\n",
    "        training_data.append(generate_data_point(sigma))\n",
    "        \n",
    "    # putting our generated data into a numpy ndarray\n",
    "    x_train = np.array(training_data)\n",
    "\n",
    "    return x_train\n",
    "\n",
    "# doing this so we do not have to calculate e\n",
    "# everytime we run our activation function tanh\n",
    "e = math.e\n",
    "\n",
    "def tanh(z):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Our activation function in the hidden layer\n",
    "    \n",
    "    Parameters:\n",
    "    z - (30 x 1) vector containing random float values\n",
    "    \n",
    "    Returns:\n",
    "    A value used for learning in the autoencoder\n",
    "    \"\"\"\n",
    "    pos_power = e ** z\n",
    "    neg_power = e ** -z\n",
    "    \n",
    "    return (pos_power - neg_power) / (pos_power + neg_power)\n",
    "\n",
    "def ReLU(z):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Our activation function in our output layer\n",
    "    \n",
    "    Parameters:\n",
    "    z - (HIDDEN_NODES x 1) vector containing random float values\n",
    "    \n",
    "    Returns:\n",
    "    A value without bounds (our predictions)\n",
    "    \"\"\"\n",
    "    \n",
    "    return max(0,max(z))\n",
    "\n",
    "def calculate_loss(x_train, x_predicted):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    calculates the loss between the train data points\n",
    "    and the predicted data points\n",
    "    \n",
    "    Parameters:\n",
    "    x_train - (5000 x 30) dimensional array\n",
    "    x_predicted - (5000 x 30) dimensional array\n",
    "    \n",
    "    Returns:\n",
    "    loss - a float value indicating our error\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of data points\n",
    "    N = len(x_train)\n",
    "    \n",
    "    # calculating the loss\n",
    "    loss = (1 / N) * np.sum( (np.linalg.norm((x_train - x_predicted)))**2 )\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def xavier_initialization(INPUT_NODES, HIDDEN_NODES, OUTPUT_NODES):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "    INPUT_NODES - an integer representing the number of input nodes in the first layer\n",
    "    HIDDEN_NODES - an integer representing the number if hidden nodes in the second layer\n",
    "    OUTPUT_NODES - an integer representing the number of output nodes in the third layer\n",
    "    \n",
    "    Returns:\n",
    "    w_vector - a ndarray or vector of shape ( (INPUT_NODES * HIDDEN_NODES) x HIDDEN_NODES)\n",
    "    \n",
    "    Purpose:\n",
    "    To intialize weights for our neural network, but in this case\n",
    "    our autoencoder\n",
    "    \"\"\"\n",
    "\n",
    "    # calculating total number of weights in our neural network (Fully Connected Layers)\n",
    "    number_of_weights = (INPUT_NODES * HIDDEN_NODES)\n",
    "\n",
    "    # building our w_vector - first build a temporary vector with all weights equal to one \n",
    "    w_vector = np.ones(number_of_weights)\n",
    "\n",
    "    # calculating the weights with Xavier intialization\n",
    "    for index in range(0, number_of_weights):\n",
    "\n",
    "        num_in = 0 # number of in nodes\n",
    "        num_out = HIDDEN_NODES # number of out nodes\n",
    "\n",
    "        a = -1 * math.sqrt(6/(num_in + num_out)) # beginning of interval\n",
    "        b = -a # end of interval \n",
    "\n",
    "        new_weight = np.random.uniform(a,b) # calculating our new weight\n",
    "        w_vector[index] = new_weight # putting our new weight into our weight vector\n",
    "\n",
    "        \n",
    "    # grouping the vector for each node\n",
    "    w_vector = np.array(np.split(w_vector, HIDDEN_NODES))\n",
    "    \n",
    "    # returning our weight vector \n",
    "    return w_vector \n",
    "\n",
    "def foward_propogation(input_layer, weights, HIDDEN_NODES):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    input_layer - ndarray of shape (INPUT_NODES x 1)\n",
    "    weights - a ndarray or vector of shape ( (INPUT_NODES * HIDDEN_NODES) x 1)\n",
    "    HIDDEN_NODES - an integer representing the number if hidden nodes in the second layer\n",
    "    \n",
    "    Returns:\n",
    "    new_hidden_layer - ndarray of shape (HIDDEN_NODES x 1)\n",
    "    new_out_layer - ndarray of shape (INPUT_NODES x 1)\n",
    "    \n",
    "    Purpose:\n",
    "    To compute a new hidden layer based off the weights of our model\n",
    "    To compute a new output layer with the newly computed hidden layer\n",
    "    \"\"\"    \n",
    "    # This function is vectorized using numpy\n",
    "    # for incredibly fast computation!!\n",
    "    \n",
    "    # one foward pass using our activation function tanh \n",
    "    new_hidden_layer = tanh(input_layer * weights)\n",
    "\n",
    "    # getting output layer with our ReLU\n",
    "    new_output_layer = np.apply_along_axis(ReLU, 1, new_hidden_layer.T)\n",
    "\n",
    "    return (new_hidden_layer, new_output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e731b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2 # 0th input layer, 1st hidden layer, 2nd output layer\n",
    "INPUT_NODES = 30 # input layer\n",
    "LIST_OF_HIDDEN_NODES = list(range(1,30+1)) # hidden layer\n",
    "OUTPUT_NODES = 30 # output layer \n",
    "HIDDEN_NODES = LIST_OF_HIDDEN_NODES[14] # number of hidden nodes in the hidden layer\n",
    "\n",
    "# generate our data for training\n",
    "x_train = generate_train_data_set()\n",
    "\n",
    "# random initialization - Xavier Edition\n",
    "weights = xavier_initialization(INPUT_NODES, HIDDEN_NODES, OUTPUT_NODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd57d4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4936.511285796989"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for input_layer in x_train:\n",
    "    hidden_layer, output_layer = foward_propogation(input_layer, weights, HIDDEN_NODES)\n",
    "\n",
    "    initial_graidents = 2 * (output_layer - input_layer)\n",
    "    hidden_gradients = np.apply_along_axis(np.sum, 1, (initial_graidents * np.dot(tanh(np.dot(weights,hidden_layer.T)), weights)).T)\n",
    "    weights = weights - (hidden_gradients * 0.0001) * np.dot(tanh(np.dot(weights, hidden_layer.T)), hidden_layer)\n",
    "    \n",
    "loss = 0\n",
    "for data_point in x_train:\n",
    "    hidden_layer, output_layer = foward_propogation(data_point, weights, HIDDEN_NODES)\n",
    "    loss += calculate_loss(data_point,output_layer)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cd368a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propogation(output_layer, input_layer, alpha):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    \n",
    "    Returns:\n",
    "    new_weights = the new computed weights of our model\n",
    "    that minimize our error\n",
    "    \n",
    "    Purpose:\n",
    "    To update the weights of our model\n",
    "    using SGD (stochastic gradient descent)\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
